# ğŸ“š FK-WEB-CRAWLER: 
### A Web Crawler project with Change Detection, Scheduler, REST API Features

A fully-featured Python-based data pipeline that:
- Crawls book data from `books.toscrape.com`
- Detects changes using fingerprinting
- Stores books in MongoDB Atlas
- Logs changes and inserts
- Schedules daily updates
- Serves data through a FastAPI-secured RESTful API with rate limiting and authentication

---

## ğŸ“ Project Structure

```
FK-CRAWLER/
â”‚
â”œâ”€â”€ api/                            # FastAPI server
â”‚   â”œâ”€â”€ auth/                       # API Key Auth
â”‚   |   â”œâ”€â”€ security.py             # Authentication
â”‚   â”œâ”€â”€ models/                     # Pydantic schemas
â”‚   |   â”œâ”€â”€ book.py                 # Pydantic model for Books
â”‚   |   â”œâ”€â”€ change.py               # Pydantic model for Logs
â”‚   |   â”œâ”€â”€ schemas.py              # Serialization functions for book and logs
â”‚   â”œâ”€â”€ routes/                     # API endpoints
â”‚   |   â”œâ”€â”€ __init__.py             # 
â”‚   |   â”œâ”€â”€ books.py                # 
â”‚   |   â”œâ”€â”€ changes.py              # 
â”‚   â”œâ”€â”€ __init__.py                 # 
â”‚   â”œâ”€â”€ main_test.py                # 
â”‚   â””â”€â”€ main.py                     # 
â”‚
â”œâ”€â”€ crawler/                        # Web scraping logic
â”‚   â”œâ”€â”€ fkcrawling/
â”‚   |   â”œâ”€â”€ spiders/                # Scrapy spiders
â”‚   |   |   â”œâ”€â”€ __init__.py         # 
â”‚   |   |   â”œâ”€â”€ book_schema.py      #
â”‚   |   |   â”œâ”€â”€ crawling_spider.py  # 
â”‚   |   |   â””â”€â”€ mongodb_client.py   #
â”‚   |   â”œâ”€â”€ __init__.py             #
â”‚   |   â”œâ”€â”€ items.py                # 
â”‚   |   â”œâ”€â”€ middlewares.py          # MongoDB insertion logic
â”‚   |   â”œâ”€â”€ pipelines.py            # MongoDB insertion logic
â”‚   |   â”œâ”€â”€ settings.py             # Scrapy config
â”‚   â””â”€â”€ â””â”€â”€ logs/
â”‚           â””â”€â”€ activity.log        #
â”‚ 
â”œâ”€â”€ scheduler/                      # Daily job scheduler
â”‚   â”œâ”€â”€ daily_scheduler.py
â”‚   â””â”€â”€ crawler_runner.py
â”‚
â”œâ”€â”€ utilities/                      # Helper utilities
â”‚   â”œâ”€â”€ reports/
â”‚   |   â”œâ”€â”€ report.json             #
â”‚   â”œâ”€â”€ db_config.py                # MongoDB client
â”‚   â”œâ”€â”€ generate_report.py          # Daily change report
â”‚   â””â”€â”€ log_config.py               # Log setup
â”‚
â”œâ”€â”€ tests/                          # Unit & integration tests (TBD)
â”‚
â”œâ”€â”€ logs/                           # Logging output
â”‚   â”œâ”€â”€ activity.log
â”‚
â”œâ”€â”€ reports/                        # Daily JSON/CSV reports
â”‚
â”œâ”€â”€ .env                            # Secure API_KEY and DB URI
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md                       # This file
â””â”€â”€ scrapy.cfg
```

---

## ğŸš€ Features

- âœ… Scrapy-powered web crawler
- âœ… MongoDB Atlas integration with deduplication
- âœ… Fingerprint-based change detection
- âœ… Change logging and raw HTML snapshot
- âœ… APScheduler-powered daily job
- âœ… Daily change reports in JSON and CSV
- âœ… RESTful FastAPI server:
  - `/books` with filtering, pagination, sorting
  - `/books/{book_id}` for book details
  - `/changes` to get recent logs
- âœ… API Key Authentication
- âœ… Rate limiting (100 req/hr per IP)
- âœ… OpenAPI (Swagger) docs

---

## ğŸ”§ Setup Instructions

### ğŸ“¦ Requirements

- Python 3.10+
- MongoDB Atlas account
- pipenv / virtualenv (recommended)

### ğŸ“ 1. Clone the Repository

```bash
git clone https://github.com/yourusername/fk-crawler.git
cd fk-crawler
```

### ğŸ“ 2. Install Dependencies

```bash
pip install -r requirements.txt
```

### âš™ï¸ 3. Create `.env` File

Create a `.env` file at the root:

```
MONGODB_URI=mongodb+srv://<user>:<pass>@cluster.mongodb.net/
API_KEY=123456
```

> âœ… `.env` is automatically loaded using `dotenv`.

---

## ğŸ•·ï¸ Run Crawler

```bash
cd crawler/fkcrawling
scrapy crawl fkcrawler
```

- Inserts books
- Logs changes (if any)
- Stores raw HTML
- Logs output to `/logs/activity.log`

---

## ğŸ—“ï¸ Run Scheduler

```bash
cd scheduler
python daily_scheduler.py
```

- Crawls every day using APScheduler
- Detects new books or changes
- Logs them in MongoDB and filesystem

---

## ğŸ§ª Run FastAPI Server

```bash
cd api
uvicorn main:app --reload
```

- API is hosted at `http://localhost:8000`
- Swagger docs: `http://localhost:8000/docs`

---

## ğŸ” API Key Usage

All endpoints are protected via API key.

### Headers:

```
FKCRAWLER-API-KEY: 123456
```

---

## ğŸ“‚ API Endpoints

| Endpoint          | Method | Description                              |
|------------------|--------|------------------------------------------|
| `/books`         | GET    | Get all books (filter, sort, paginate)   |
| `/books/{id}`    | GET    | Get book by MongoDB ObjectId             |
| `/changes`       | GET    | Get recent changes                       |
| `/docs`          | GET    | Swagger UI (OpenAPI spec)                |

---

## ğŸ“¤ Daily Report Output

On successful run, you'll get:

```bash
/reports/
â”œâ”€â”€ change_report_YYYY-MM-DD.json
â”œâ”€â”€ change_report_YYYY-MM-DD.csv
```

Includes:
- New insertions
- Fields changed
- Source URLs and timestamps

---

## ğŸ§ª Testing

Unit and integration tests will be added in `/tests/`.

Planned with `pytest`, `mongomock`, and `httpx` for:
- API endpoints
- DB operations
- Crawling output
- Scheduler jobs

---

## ğŸ’¡ Sample MongoDB Document

```json
{
  "_id": ObjectId("123..."),
  "name": "A Light in the Attic",
  "price_with_tax": 12.99,
  "availability": "In stock",
  "rating": 3,
  "source_url": "https://books.toscrape.com/catalogue/.../index.html",
  "raw_html": "<html>...</html>",
  "fingerprint": "abc123...",
  "crawl_timestamp": "2025-06-27T10:00:00Z"
}
```

---

## ğŸ§¾ Deliverables Checklist (from PDF âœ…)

| Requirement                               | Status     |
|------------------------------------------|------------|
| âœ… Crawler using Scrapy                  | Done       |
| âœ… Scheduler with change detection       | Done       |
| âœ… Change log storage                    | Done       |
| âœ… FastAPI server                        | Done       |
| âœ… API key + rate limiting               | Done       |
| âœ… Swagger UI                            | Done       |
| âœ… `.env` support                        | Done       |
| âœ… Daily reports (JSON + CSV)            | Done       |
| âœ… Screenshot/logs of scheduler/crawler  | âœ”ï¸ See `/logs` |
| âœ… Folder structure & README             | âœ… This file |

---

## ğŸ“¬ Postman / Swagger UI

Use [http://localhost:8000/docs](http://localhost:8000/docs) to interactively test all endpoints.

Or import a Postman collection (see repo if added).

---

## ğŸ§  Future Improvements

- Add unit + integration tests
- Dockerize for consistent environments
- Add email alerts for major changes
- Add export formats: PDF, Excel

---

## ğŸ§‘â€ğŸ’» Author

**Sadman Alvee**  
ğŸ“§ alvee@example.com  
ğŸ”— [GitHub](https://github.com/sadmanalvee)

---

## ğŸ“„ License

MIT License â€“ feel free to use, improve, and contribute!
